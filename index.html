<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Document Summarization by annepuharsha</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Document Summarization</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/annepuharsha/Project-ML-DocumentSummarization" class="btn">View on GitHub</a>
      <a href="https://github.com/annepuharsha/Project-ML-DocumentSummarization/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/annepuharsha/Project-ML-DocumentSummarization/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="document-summarization-using-a-machine-learning-approach" class="anchor" href="#document-summarization-using-a-machine-learning-approach" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Document Summarization using a Machine Learning approach</h1>

<h2>
<a id="introductory-presentation" class="anchor" href="#introductory-presentation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introductory Presentation:</h2>

<p>Please follow this link to the RoadMap Presentation - <a href="https://github.com/annepuharsha/Project-ML-DocumentSummarization/blob/master/DocSummarization-ML.pdf">DocSummarization-ML</a></p>

<hr>

<h2>
<a id="pre-processing" class="anchor" href="#pre-processing" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pre-Processing:</h2>

<p>The pre-processing is being done using the following steps -
 <img src="./Pics/Pre-Processing.png" alt=""></p>

<hr>

<h2>
<a id="features-identified" class="anchor" href="#features-identified" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Features Identified:</h2>

<p>The summarizer produces summaries using a feature profile oriented sentence extraction stategy. The following features have been identified as important:</p>

<pre><code>1)  CUE PHRASES
2)  SIMILARITY TO THE TITLE
3)  TF-ISF
4)  DEGREE CENTRALITY
5)  C-LEX RANK
6)  TEXT RANK
7)  NON-ESSENTIAL WORDS (Like "Additionally")
8)  NUMERICAl DATA
9)  SENTENCE LENGTH
10) WORDNET BASED RANKING
</code></pre>

<hr>

<h2>
<a id="feature-extraction" class="anchor" href="#feature-extraction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Feature Extraction</h2>

<p><strong>Mean TF-ISF</strong> </p>

<p>This feature is analogous to TF-IDF in the context of Information Retrieval. In IR, we have to select few documents which are most relevant from a given set of documents. Here, we have to select few sentences from the given document.The used feature is calculated as the mean value of the TF-ISF measure for all the words of each sentence. The feature is calculated in the following way-</p>

<p><img src="./Pics/TF-ISF.png" alt="">  </p>

<p><strong>Sentence Length</strong> </p>

<p>Sentences of too short length add little value when included in the summary. This feature is used to penalize such sentences so that they will not be included in final summary.</p>

<p><strong>Sentence Position</strong> </p>

<p>Sentences at the start of the document and also at the end of the document generally have more importance and are candidates to be included in the summary. Even, sentence position in each paragraph is useful, but here the summarizer checks for the overall position and weights accordingly (as it summarizes news articles which generally tend to have information about a single aspect and not divided into paragraphs - Identified from the corpus present with us). This is the graph which shows the normalized weights assigned to sentences-</p>

<p><img src="./Pics/SentencePosition.png" alt=""></p>

<p><strong>Wordnet Ranking</strong></p>

<p>We created list of Keywords by taking Adjectives,nouns and Propernouns.We have calculated number of times a particular word or its synonyms occured in document by using synset.Now if Keyword is a properNoun a score of 1 is assigned or else a highest similarity score is assigned which is calculated with respect to synonyms of other words present in document.Now for each sentence a score is assigned which is (sum of scores of all keywords present in the sentence)/(Number of keywords present in sentence).</p>

<p><strong>Numerical Data</strong></p>

<p>Sentences containing numerical data have higher chance of getting included in the extractive summary of the document. This feature emphasis on the numerical data present in the sentences. The numerical data generally correspond to some computed values or results which are important to be included in the summary. So, the sentences having numerical data, we add extra score to them.</p>

<p><strong>Proper Nouns</strong></p>

<p>Sentences containing proper nouns are having greater chance of getting included in summary. The motivation for this feature is that the occurrence of proper names, referring to people and places, are clues that a sentence is relevant for the summary. This is considered here as a binary feature, indicating whether a sentence  contains (value "true") at least one proper name or not (value "false"). Proper names were detected by a part of speech tagger.</p>

<p><strong>Text Rank</strong></p>

<p>The TextRank algorithm exploits the structure of the text itself to determine keyphrases that appear "central" to the text in the same way that PageRank selects important Web pages. TextRank is a general purpose graph-based ranking algorithm for NLP. For keyphrase extraction, we built a graph using some set of text units as vertices. Edges are based
on some measure of semantic or lexical similarity between the text unit vertices. Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity. The proposed graph based text ranking algorithm consists of three steps - word frequency analysis; word positional and string pattern based weight calculation algorithm; ranking the sentences by normalizing the results above</p>

<hr>

<h2>
<a id="training-the-summarizer" class="anchor" href="#training-the-summarizer" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Training the summarizer</h2>

<p>The summarizer was trained with a set of documents and their respective gold standard summaries using supervised learning algorithms. To accomplish this, a classification function that estimates the probability of a sentence being selected in the summary was developed. We were given with a set of training documents, their respective summaries and a feature vector file for each of those training documents. A feature vector file was containing feature vectors of all the sentences of the respective training document. A feature vector is a tuple of feature values of a sentence. We combined the feature vectors from all those files and divided them into two classes. First one, containing the feature vectors of the sentences
that are selected into the summary and the other containing the feature vectors of the sentences which are not selected into the summary. Using the feature vectors in a class, the variance and standard deviation of the values for each feature have been computed for that class.</p>

<hr>

<h2>
<a id="summary-generation" class="anchor" href="#summary-generation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Summary generation</h2>

<p>A feature vector file is prepared for the given input document with a feature vector for each of its sentence. Now the feature vectors of these are classified into one of the two classes mentioned above, using naive bayes classifier. The variance and standard deviation values that were learned in the training phase are used for calculating the probabilities that involved in the naive bayes formula (probability formula of normal distribution involving variance and standard distribution is used). The sentences whose feature vectors fall into the first class make up the output summary. </p>

<hr>

<h2>
<a id="rouge-results" class="anchor" href="#rouge-results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>ROUGE Results</h2>

<p>The ROUGE evaluation toolkit is employed to evaluate the proposed algorithm. ROUGE, an automated summarization evaluation package based on Ngram statistics, is found to be highly correlated with human evaluations. The evaluations are reported in ROUGE-1 metrics, which seeks unigram matches between the generated and the reference summaries. The ROUGE-1 metric is found to have high correlation with human judgments at a 95% confidence level and hence used for evaluation.</p>

<p>The results obtained for the summarizer developed are shown below.</p>

<p><img src="./Pics/Large.png" alt=""></p>

<p><img src="./Pics/Small.png" alt=""></p>

<hr>

<h2>
<a id="final-presentation" class="anchor" href="#final-presentation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Final Presentation:</h2>

<p>Please follow this link to the Final Presentation - <a href="https://github.com/annepuharsha/Project-ML-DocumentSummarization/blob/master/JusticeLeague_PPT.pdf">DocSummarization-ML-Final-PPT</a></p>

<hr>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/annepuharsha/Project-ML-DocumentSummarization">Document Summarization</a> is maintained by <a href="https://github.com/annepuharsha">annepuharsha</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
